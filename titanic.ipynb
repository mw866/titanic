{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "# from pyspark import SparkConf\n",
    "\n",
    "# import plotly \n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import Layout\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intitialize SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_df = (sqlContext\n",
    "                 .read\n",
    "                 .format('csv')\n",
    "                 .options(header='true', inferSchema='true')\n",
    "                 .load('./data/train.csv'))\n",
    "train_data_df.cache()\n",
    "train_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df\n",
    "# Same as display(train_data_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "What is something that you noticed that could cause issues?\n",
    "Hopefully, you noticed that some data objects (objects here are rows which represent a passenger) have a null age. This is going to cause some issues. How can we fix these non-values without affecting any population properties of the data? Let's use some [**domain knowledge**](http://www.simafore.com/blog/the-value-of-domain-knowledge-in-data-science) to do some more data exploration. Let's see if age follows any trends based off of class and/or ticket fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pclass: int, Age: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_age_df = train_data_df.orderBy('Age', ascending=True)\n",
    "\n",
    "avg_age_df = explore_age_df.where(explore_age_df['Age'].isNotNull()).groupBy('Pclass').avg('Age')\n",
    "avg_age_df = avg_age_df.select('Pclass', avg_age_df['avg(Age)'].alias('Age'))\n",
    "avg_age_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing Age values with the average age per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "avg_age_list = avg_age_df.collect()\n",
    "\n",
    "# Replace null values with the average age values from our passenger class list\n",
    "data_with_age_df = (train_data_df\n",
    "                     .select('*', \n",
    "                             when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 1), \n",
    "                                  avg_age_list[0].Age)\n",
    "                             .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 2), \n",
    "                                             avg_age_list[1].Age)\n",
    "                                        .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 3), \n",
    "                                                        avg_age_list[2].Age)\n",
    "                                                   .otherwise(col('Age')))).alias('FilledAge')))\n",
    "\n",
    "# Replace the Age column values with those from our FilledAge column and then drop FilledAge.\n",
    "data_with_age_df = data_with_age_df.withColumn('Age', data_with_age_df['FilledAge']).drop('FilledAge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions\n",
    "def sex_to_int(sex):\n",
    "  if(sex.lower() == 'male'):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "sex_classify = functions.udf(sex_to_int, IntegerType())\n",
    "sex_int_df = data_with_age_df.select('*', sex_classify(data_with_age_df['Sex']).alias('IntSex'))\n",
    "data_sex_indexed_df = sex_int_df.withColumn('Sex', sex_int_df['IntSex']).drop('IntSex').cache()\n",
    "\n",
    "data_sex_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: int, Embarked: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cabin_to_int(cabin):\n",
    "    if cabin:\n",
    "        return ord(cabin[0])-ord('A')+1 #A:1; B:2; C:3; D:4; None:0\n",
    "    else:\n",
    "        return 0\n",
    "cabin_classify = functions.udf(cabin_to_int, IntegerType())\n",
    "\n",
    "cabin_int_df = data_sex_indexed_df.select('*', cabin_classify(data_sex_indexed_df['Cabin']).alias('IntCabin'))\n",
    "data_cabin_indexed_df = cabin_int_df.withColumn('Cabin', cabin_int_df['IntCabin']).drop('IntCabin').cache()\n",
    "\n",
    "data_cabin_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Index Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: int, Embarked: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embarked_to_int(embarked):\n",
    "    if embarked:\n",
    "        return ord(embarked) #TBD\n",
    "    else:\n",
    "        return 0\n",
    "embarked_classify = functions.udf(embarked_to_int, IntegerType())\n",
    "\n",
    "embarked_int_df = data_cabin_indexed_df.select('*', embarked_classify(data_sex_indexed_df['Embarked']).alias('IntEmbarked'))\n",
    "data_embarked_indexed_df = embarked_int_df.withColumn('Embarked', embarked_int_df['IntEmbarked']).drop('IntEmbarked').cache()\n",
    "\n",
    "data_embarked_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[PassengerId: int, label: int, features: vector],\n",
       " DataFrame[PassengerId: int, label: int, features: vector])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'], outputCol='features'\n",
    ")\n",
    "\n",
    "data = assembler.transform(data_embarked_indexed_df).select(col('PassengerId'),col('Survived').alias('label'),'features')\n",
    "splits = data.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache() #significant ~30% improvement to fitting\n",
    "test = splits[1].cache()\n",
    "train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 4\n",
    "### build at least two of {Naïve Bayes, Logistic Regression, random forests, support vector machines or neural networks using the libraries of Spark.MLLib only. \n",
    "Please refer to the code below.\n",
    "### Explain your choice; \n",
    "### plot learning curves; \n",
    "### explain observed behavior; \n",
    "### investigate which features are most informative; \n",
    "\n",
    "As the baseline, the following features are chosen based on common sense ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']. \n",
    "\n",
    "Then the individual features are removed to measure the (negative) impact to the prediction performance.\n",
    "\n",
    "\n",
    "\n",
    "**Accuracy**|**Baseline**|**-'Pclass'**|**- 'Sex'**|**- 'Age'**|**-'SibSp'**|**-'Parch'**|**-'Fare'**|**-'Embarked'**\n",
    ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
    "NaiveBayes                    |0.72826087|0.710382514|0.646153846|0.674157303|0.683544304|0.67816092|0.803108808|0.709677419\n",
    "LogisticRegression            |0.804347826|0.808743169|0.687179487|0.792134831|0.797468354|0.827586207|0.808290155|0.779569892\n",
    "RandomForestClassifier        |0.831521739|0.825136612|0.733333333|0.814606742|0.816455696|0.844827586|0.808290155|0.790322581\n",
    "MultilayerPerceptronClassifier|0.809782609|0.759562842|0.687179487|0.837078652|0.689873418|0.643678161|0.792746114|0.784946237\n",
    "\n",
    "\n",
    "Based on the results above, below is the ranking of the most informative features's:\n",
    "\n",
    "* Sex\n",
    "* Age\n",
    "* Pclass\n",
    "* SibSp\n",
    "* Embarked\n",
    "* Parch\n",
    "* Fare\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### do at least one round of error analysis to maximize your chosen metric (F1, accuracy, weighted F1); \n",
    "\n",
    "The prediction accuracy using features: \n",
    "\n",
    "    Features: 'Cabin','Sex','Age','SibSp','Parch','Fare','Embarked'\n",
    "    NaiveBayes                    \t0.710382513661\n",
    "    LogisticRegression            \t0.808743169399\n",
    "    RandomForestClassifier        \t0.825136612022\n",
    "    MultilayerPerceptronClassifier\t0.75956284153\n",
    "\n",
    "The replacement of Cablin with Pclass improves the \n",
    "\n",
    "    Features: 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'\n",
    "    NaiveBayes                    \t0.728260869565\n",
    "    LogisticRegression            \t0.804347826087\n",
    "    RandomForestClassifier        \t0.83152173913\n",
    "    MultilayerPerceptronClassifier\t0.809782608696\n",
    "\n",
    "### explain your choice of metric.\n",
    "The accuracy is the number of correct predictions made divided by the total number of predictions made. It is chosen because it's a good performance indicator of the prediction and corralates well with the F1/weighted F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes                    \t0.632768361582\n",
      "LogisticRegression            \t0.80790960452\n",
      "RandomForestClassifier        \t0.80790960452\n",
      "MultilayerPerceptronClassifier\t0.655367231638\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "for classifier in (NaiveBayes, LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier): # '[]' won't work\n",
    "    \n",
    "    if classifier != MultilayerPerceptronClassifier:\n",
    "        model = classifier()\n",
    "    else:\n",
    "        #Number of inputs = the size of feature vectors. Number of outputs = the total number of labels.\n",
    "        features_size = data.select(\"features\").first()[0].size\n",
    "        model = classifier(layers=[features_size,10,2]) \n",
    "    model_trained = model.fit(train)\n",
    "\n",
    "    test_predicted = model_trained.transform(test)\n",
    "    #test_predicted.show(10)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\") \n",
    "    accuracy = evaluator.evaluate(test_predicted, {evaluator.metricName: \"accuracy\"}) # f1|weightedPrecision|weightedRecall|accuracy\n",
    "    print(\"\"+classifier.__name__.ljust(30) + '\\t' + str(accuracy))\n",
    "    \n",
    "#     print('Wrong predictions for error analysis')\n",
    "#     test_predicted.filter(test_predicted['prediction'] != test_predicted['label']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Titanic Tutorial",
  "notebookId": 338688711146068
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
