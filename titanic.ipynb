{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "# from pyspark import SparkConf\n",
    "\n",
    "# import plotly \n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import Layout\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intitialize SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_df = (sqlContext\n",
    "                 .read\n",
    "                 .format('csv')\n",
    "                 .options(header='true', inferSchema='true')\n",
    "                 .load('./data/train.csv'))\n",
    "train_data_df.cache()\n",
    "train_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df\n",
    "# Same as display(train_data_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "What is something that you noticed that could cause issues?\n",
    "Hopefully, you noticed that some data objects (objects here are rows which represent a passenger) have a null age. This is going to cause some issues. How can we fix these non-values without affecting any population properties of the data? Let's use some [**domain knowledge**](http://www.simafore.com/blog/the-value-of-domain-knowledge-in-data-science) to do some more data exploration. Let's see if age follows any trends based off of class and/or ticket fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pclass: int, Age: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_age_df = train_data_df.orderBy('Age', ascending=True)\n",
    "\n",
    "avg_age_df = explore_age_df.where(explore_age_df['Age'].isNotNull()).groupBy('Pclass').avg('Age')\n",
    "avg_age_df = avg_age_df.select('Pclass', avg_age_df['avg(Age)'].alias('Age'))\n",
    "avg_age_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing Age values with the average age per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "avg_age_list = avg_age_df.collect()\n",
    "\n",
    "# Replace null values with the average age values from our passenger class list\n",
    "data_with_age_df = (train_data_df\n",
    "                     .select('*', \n",
    "                             when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 1), \n",
    "                                  avg_age_list[0].Age)\n",
    "                             .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 2), \n",
    "                                             avg_age_list[1].Age)\n",
    "                                        .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 3), \n",
    "                                                        avg_age_list[2].Age)\n",
    "                                                   .otherwise(col('Age')))).alias('FilledAge')))\n",
    "\n",
    "# Replace the Age column values with those from our FilledAge column and then drop FilledAge.\n",
    "data_with_age_df = data_with_age_df.withColumn('Age', data_with_age_df['FilledAge']).drop('FilledAge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions\n",
    "def sex_to_int(sex):\n",
    "  if(sex.lower() == 'male'):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "sex_classify = functions.udf(sex_to_int, IntegerType())\n",
    "sex_int_df = data_with_age_df.select('*', sex_classify(data_with_age_df['Sex']).alias('IntSex'))\n",
    "data_sex_indexed_df = sex_int_df.withColumn('Sex', sex_int_df['IntSex']).drop('IntSex').cache()\n",
    "\n",
    "data_sex_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: int, Embarked: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cabin_to_int(cabin):\n",
    "    if cabin:\n",
    "        return ord(cabin[0])-ord('A')+1 #A:1; B:2; C:3; D:4; None:0\n",
    "    else:\n",
    "        return 0\n",
    "cabin_classify = functions.udf(cabin_to_int, IntegerType())\n",
    "\n",
    "cabin_int_df = data_sex_indexed_df.select('*', cabin_classify(data_sex_indexed_df['Cabin']).alias('IntCabin'))\n",
    "data_cabin_indexed_df = cabin_int_df.withColumn('Cabin', cabin_int_df['IntCabin']).drop('IntCabin').cache()\n",
    "\n",
    "data_cabin_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Index Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: int, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: int, Embarked: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embarked_to_int(embarked):\n",
    "    if embarked:\n",
    "        return ord(embarked) #TBD\n",
    "    else:\n",
    "        return 0\n",
    "embarked_classify = functions.udf(embarked_to_int, IntegerType())\n",
    "\n",
    "embarked_int_df = data_cabin_indexed_df.select('*', embarked_classify(data_sex_indexed_df['Embarked']).alias('IntEmbarked'))\n",
    "data_embarked_indexed_df = embarked_int_df.withColumn('Embarked', embarked_int_df['IntEmbarked']).drop('IntEmbarked').cache()\n",
    "\n",
    "data_embarked_indexed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[label: int, features: vector],\n",
       " DataFrame[label: int, features: vector])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Sex','Age','SibSp','Parch','Fare','Cabin','Embarked'], outputCol='features'\n",
    ")\n",
    "\n",
    "data = assembler.transform(data_embarked_indexed_df).select(col('Survived').alias('label'),'features')\n",
    "splits = data.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache() #significant ~30% improvement to fitting\n",
    "test = splits[1].cache()\n",
    "train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 ms, sys: 1.82 ms, total: 8.83 ms\n",
      "Wall time: 760 ms\n",
      "NaiveBayes: \t Test set accuracy = 0.685\n",
      "===\n",
      "CPU times: user 17.1 ms, sys: 4.78 ms, total: 21.9 ms\n",
      "Wall time: 1.19 s\n",
      "LogisticRegression: \t Test set accuracy = 0.77\n",
      "===\n",
      "CPU times: user 9.75 ms, sys: 2.41 ms, total: 12.2 ms\n",
      "Wall time: 510 ms\n",
      "RandomForestClassifier: \t Test set accuracy = 0.785\n",
      "===\n",
      "CPU times: user 21.7 ms, sys: 6.87 ms, total: 28.5 ms\n",
      "Wall time: 2.43 s\n",
      "MultilayerPerceptronClassifier: \t Test set accuracy = 0.74\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "for classifier in (NaiveBayes, LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier): # '[]' won't work\n",
    "    \n",
    "    if classifier != MultilayerPerceptronClassifier:\n",
    "        model = classifier()\n",
    "    else:\n",
    "        #Number of inputs = the size of feature vectors. Number of outputs = the total number of labels.\n",
    "        model = classifier(layers=[7,15,2]) \n",
    "    %time model_trained = model.fit(train)\n",
    "\n",
    "    test_predicted = model_trained.transform(test)\n",
    "    #test_predicted.show(10)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(test_predicted)\n",
    "    print(\"\"+classifier.__name__+\": \\t Test set accuracy = \" + str(accuracy)+\"\\n===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Titanic Tutorial",
  "notebookId": 338688711146068
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
